{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project _2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM5lnamt6RL1Q/RmxCyqj0W",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CCIR-Academy/Techcamp2021S-Phase3/blob/main/Project__2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95c7qtaSGT78"
      },
      "source": [
        "# Project 2: Further on NLP\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYch7F1K5f8t"
      },
      "source": [
        "## Intro\n",
        "- As we progress to the final section of the Techcamp, we will attempt to explore further on NLP: we would start to take some of the more analytic approaches with regards to analyzing and processing text data that consist of natural language; such approaches are mostly inspired by theories and practices in the field of Linguistic Studies and powered by deep-learning with the help of modern hardwares, and the goal is to handle these data with adequate understanding of dynamics of human verbal communication as well as the contextual meaning endowed by human in the actual usage of language.  \n",
        "- In this project, we will be using Colab as our environment for the sake of a more controlled and powerful runtime that could provide more consistent experience which might vary by a great degree otherwise for numerous reasons.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVCHa9ms7y8m"
      },
      "source": [
        "## Expectation\n",
        "1. Similar to where we were in the last project, the capability behind the common applications of NLP is brought to you by researchers and developers on the basis of substantial mathematical and linguistic theories and breakthroughs, which could be overwelming for people that are new to machine learning in general; as such, our attention for the lecturing and coding sessions are somewhat limited to the implementations of a series of more out-of-box applications provided by some of the more production-oriented libraries like `spaCy`.\n",
        "2. However, to some extent (in comparison to the previous project), the mechanisms, algorithms, and parameters involved in such applications seem rather \"blackbox\" to us, meaning that we don't have many ways to tweak and try it out other than simply feeding it with more data.\n",
        "3. To address this issue for the purposing of making the course experience more tangible and aspiring, we are adopting a few strategies in our exploration:\n",
        "    - We would like encourage you to integrate the NLP features with what we have covered in developing and deploying a bot on `Chai.ml` to substantially provide a complete \"playable\" experience.\n",
        "    - We would like to encourage you to explore freely at [spaCy Univererse](https://spacy.io/universe), which is a collection of many great resources developed with or for `spaCy`, to get inspirations for what you would like to do after you learn something interesting about NLP. In terms of topic, such projects are encouraged to be stemming from your own interests or demand in any aspects of life, as long as you feel driven and enjoying the process of developing it (say, automatically create a good meme XD).\n",
        "4. A good place to start with would be accessing the resources from a community called `HuggingFace Hub`. You can learn more about it here: https://spacy.io/usage/projects#huggingface_hub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uroe5EwF60tt"
      },
      "source": [
        "## Part 1: Spam mail classification\n",
        "> Note: This part is heavily inspired by this blog article: https://dataaspirant.medium.com/how-to-build-an-effective-email-spam-classification-model-with-spacy-python-af2217bf4e30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxRFm064G7dn"
      },
      "source": [
        "In this part of the project, we will attempt to use the default configurations to train a local model from a public dataset from Kaggle. Here is the link of the dataset as you will need to download it and upload it to the Colab runtime: https://www.kaggle.com/venky73/spam-mails-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlIU6Ct9az2U"
      },
      "source": [
        "### Preparation\n",
        "1. You would need to keep your code and other files updated on GitHub under the group CCIR-Academy. You can do so by either creating a new repo or keep using the repo for your previous sections. \n",
        "2. For your code to be able to access the dataset, you would need to either upload it to the runtime or store it to Google Drive and mount it to the runtime. Notice that if the runtime would not store the uploaded file after the runtime has been terminated (e.g., after you close the web page for Colab), and you might need to upload it again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR1j9VxwN7in"
      },
      "source": [
        "### Loading the dataset\n",
        "At the begining of our entry function `main()`, We would use `pandas` to load our dataset as recommended by `spaCy` like many other machine learning libraries.  Before doing so, we will define the basic parameters casually by defining them as common variables. We would also like to print a line to see if the dataset has been loaded properly.\n",
        "```python\n",
        "## Paths\n",
        "data_path = \"/content/spam_ham_dataset.csv\"\n",
        "\n",
        "def main():\n",
        "    # Load dataset\n",
        "    data = pd.read_csv(data_path)\n",
        "    observations = len(data.index)\n",
        "    print(\"Dataset Size: {}\".format(observations))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iXMATZYPiiK"
      },
      "source": [
        "### Creating a blank spaCy model and a simple Pipeline\n",
        "- To understand the dataset on the level of a natural language, a typical model would take multiple steps to process the data to the degree that it can make informed predictions or evaluation based on the processed data. On one hand, with trained models, we can directly get some output by providing some input; on the other hand, we can also train a new model by providing a dataset with both the input and the expected output.\n",
        "- In our project, we create a blank model a pipeline from the \"textcat\" template which can conduct text categorizations after appropriate training.\n",
        "```python\n",
        "    # Create an empty spacy model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Create the TextCategorizer with exclusive classes and \"bow\" architecture\n",
        "    text_cat = nlp.create_pipe(\n",
        "                  \"textcat\",\n",
        "                  config={\n",
        "                    \"exclusive_classes\": True,\n",
        "                    \"architecture\": \"bow\"})\n",
        "\n",
        "    # Adding the TextCategorizer to the created empty model\n",
        "    nlp.add_pipe(text_cat)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCNutG9mmn9Y"
      },
      "source": [
        "### Prepare for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyqikAlnchRt"
      },
      "source": [
        "### Appendix: Sample Code (For the purpose of instruction and self-learning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MiAnXyd4I7FP"
      },
      "source": [
        "%pip install pandas spacy seaborn sklearn matplotlib\n",
        "\n",
        "\"\"\"\n",
        "===============================================\n",
        "Objective: Building email classifier with spacy\n",
        "Author: saimadhu.polamuri\n",
        "Blog: dataaspirant.com\n",
        "Date: 2020-07-17\n",
        "===============================================\n",
        "\"\"\"\n",
        " \n",
        "## Required packages\n",
        "import random\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from spacy.util import minibatch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "## Paths\n",
        "data_path = \"/content/spam_ham_dataset.csv\"\n",
        "\n",
        "## Configurations\n",
        "sns.set(style=\"darkgrid\")\n",
        "\n",
        "def train_model(model, train_data, optimizer, batch_size, epochs=10):\n",
        "    losses = {}\n",
        "    random.seed(1)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        random.shuffle(train_data)\n",
        "\n",
        "        batches = minibatch(train_data, size=batch_size)\n",
        "        for batch in batches:\n",
        "            # Split batch into texts and labels\n",
        "            texts, labels = zip(*batch)\n",
        "\n",
        "            # Update model with texts and labels\n",
        "            model.update(texts, labels, sgd=optimizer, losses=losses)\n",
        "        # print(\"Loss: {}\".format(losses['textcat']))\n",
        "\n",
        "    return losses['textcat']\n",
        "def get_predictions(model, texts):\n",
        "    # Use the model's tokenizer to tokenize each input text\n",
        "    docs = [model.tokenizer(text) for text in texts]\n",
        "\n",
        "    # Use textcat to get the scores for each doc\n",
        "    textcat = model.get_pipe('textcat')\n",
        "    scores, _ = textcat.predict(docs)\n",
        "\n",
        "    # From the scores, find the label with the highest score/probability\n",
        "    predicted_labels = scores.argmax(axis=1)\n",
        "    predicted_class = [textcat.labels[label] for label in predicted_labels]\n",
        "\n",
        "    return predicted_class\n",
        "\n",
        "\n",
        "######## Main method ########\n",
        "\n",
        "def main():\n",
        "    # Load dataset\n",
        "    data = pd.read_csv(data_path)\n",
        "    observations = len(data.index)\n",
        "    print(\"Dataset Size: {}\".format(observations))\n",
        "\n",
        "    # Create an empty spacy model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Create the TextCategorizer with exclusive classes and \"bow\" architecture\n",
        "    text_cat = nlp.create_pipe(\n",
        "                  \"textcat\",\n",
        "                  config={\n",
        "                    \"exclusive_classes\": True,\n",
        "                    \"architecture\": \"bow\"})\n",
        "\n",
        "    # Adding the TextCategorizer to the created empty model\n",
        "    nlp.add_pipe(text_cat)\n",
        "\n",
        "    # Add labels to text classifier\n",
        "    text_cat.add_label(\"ham\")\n",
        "    text_cat.add_label(\"spam\")\n",
        "\n",
        "    # Split data into train and test datasets\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "        data['text'], data['label'], test_size=0.33, random_state=7)\n",
        "    \n",
        "    train_labels = [{'cats': {'ham': label == 'ham',\n",
        "                              'spam': label == 'spam'}}  for label in y_train]\n",
        "    test_labels = [{'cats': {'ham': label == 'ham',\n",
        "                          'spam': label == 'spam'}}  for label in y_test]\n",
        "\n",
        "    # Spacy model data\n",
        "    train_data = list(zip(x_train, train_labels))\n",
        "    test_data = list(zip(x_test, test_labels))\n",
        "\n",
        "    # Model configurations\n",
        "    optimizer = nlp.begin_training()\n",
        "    batch_size = 5\n",
        "    epochs = 10\n",
        "\n",
        "    # Training the model\n",
        "    train_model(nlp, train_data, optimizer, batch_size, epochs)\n",
        "\n",
        "    # Sample predictions\n",
        "    # print(train_data[0])\n",
        "    # sample_test = nlp(train_data[0][0])\n",
        "    # print(sample_test.cats)\n",
        "\n",
        "    # Train and test accuracy\n",
        "    train_predictions = get_predictions(nlp, x_train)\n",
        "    test_predictions = get_predictions(nlp, x_test)\n",
        "    train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "\n",
        "    print(\"Train accuracy: {}\".format(train_accuracy))\n",
        "    print(\"Test accuracy: {}\".format(test_accuracy))\n",
        "\n",
        "    # Creating the confusion matrix graphs\n",
        "    cf_train_matrix = confusion_matrix(y_train, train_predictions)\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.heatmap(cf_train_matrix, annot=True, fmt='d')\n",
        "\n",
        "    cf_test_matrix = confusion_matrix(y_test, test_predictions)\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.heatmap(cf_test_matrix, annot=True, fmt='d')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlcpL7a0TGpO"
      },
      "source": [
        "## Part 2: Sentiment Analysis\n",
        "\n",
        "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews\n",
        "\n",
        "https://spacy.io/usage/rule-based-matching#example3\n",
        "\n",
        "https://realpython.com/sentiment-analysis-python/"
      ]
    }
  ]
}